{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64120e0a",
   "metadata": {},
   "source": [
    "# Cerebellar-driven cortical dynamics\n",
    "Welcome to the \"cerebellar-driven cortical dynamics\" lab! In this lab we will create an artificial neural network based on the cortico-cerebellar loop in the brain and do some fun experiments with it. Using the model, we will show that cerebellar plasticity alone is often enough to drive efficient task dynamics. We will also use the model to replicate *in silico* some exciting recent experimental results. For more context for this lab, see [this preprint paper](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B9nk9MQAAAAJ&citation_for_view=B9nk9MQAAAAJ:eQOLeE2rZwMC). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3f0db",
   "metadata": {},
   "source": [
    "## Background: model architecture and plasticity rules\n",
    "<img src=\"https://user-images.githubusercontent.com/35163856/202711207-297cb6db-2a4f-4b1f-989f-f76600c2d1ee.png\" width=\"900\">\n",
    "\n",
    "In this study we will model the temporal dynamics of the brain and how it can solve time-dependent tasks. We separate the model into two distinct components: a **cortical** component - which models some region of the neocortex -- and a **cerebellar** component -- which models some region of the cerebellum. \n",
    "\n",
    "### (Cortical only) no feedback model\n",
    "\n",
    "Cortical circuits, particularly in higher brain areas responsible for solving tasks, are highly recurrent. For this reason we model the cortical network as a recurrent neural network (RNN), where the RNN state $h_t$ represents the 'hidden' cortical state at time $t$. The RNN takes external input $x_t$, and the final cortical output $z_t$ is a linear readout of the hidden state. Let $W_{ih}$, $W_{hh}$ and $W_\\mathrm{rdt}$ denote the input, recurrent and readout weights (synapses) of the cortical network respectively. Then the dynamics of the network can be formally described with\n",
    "\n",
    "$h_t = \\alpha h_{t-1} + W_{hh}f(h_{t-1}) + W_{ih} x_t$ \n",
    "\n",
    "$ z_t = W_{\\mathrm{rdt}}f(h_t)$\n",
    "\n",
    "Where $\\alpha$ denotes the leak of the RNN which is dependent on the membrane time constant of the cortical neurons, and $f$ is the activation function of the RNN which we take as $f(x) = \\tanh(x)$.\n",
    "\n",
    "The above equations are the standard implementation of a leaky RNN and reflect the 'no feedback' architecture in the above figure. \n",
    "\n",
    "### (Cortical only) readout feedback model\n",
    "\n",
    "In this lab we will examine the role that feedback can play onto an RNN. The most obvious way to incorporate feedback is to send the cortical output $z_t$ back into the RNN. This architecture is represented in the 'readout feedback' model in the above figure, and is used in various works in the 2000s including in [echo-state networks](https://www.science.org/doi/10.1126/science.1091277) and the [FORCE](https://pubmed.ncbi.nlm.nih.gov/19709635/) algorithm. Letting $W_{\\mathrm{out},h}$ denote the feedback weights from the cortical output to the RNN, the hidden cortical state now evolves according to\n",
    "\n",
    "$h_t = \\alpha h_{t-1} + W_{hh}f(h_{t-1}) + W_{ih} x_t + W_{\\mathrm{out}-h} z_{t-1}$ \n",
    "\n",
    "This is an important control architecture to consider whilst examining the role of cerebellar feedback. \n",
    "\n",
    "### Cerebellar feedback model\n",
    "\n",
    "The novel architecture that we will examine! In this scheme we attach a cerebellar network to the cortical network; the cerebellar network receives a copy of cortical activity, and sends back a cerebellar prediction. This architecture resembles the cortico-cerebellar loop and is the 'cerebellar feedback' model in the above figure. Letting $\\mathcal{C}$ denote the cerebellar computation and $W_{\\mathcal{C}h}$ the cerebellar-cortico weights, the temporal dynamics then run according to \n",
    "\n",
    "$h_t = \\alpha h_{t-1} + W_{hh}f(h_{t-1}) + W_{ih} x_t + W_{\\mathcal{C}h} c_{t}$ \n",
    "\n",
    "$c_{t} = \\mathcal{C}(f(h_{t-1}))$\n",
    "\n",
    "What is the cerebellar computation $\\mathcal{C}$? We approximate cerebellar processing with two main stages of forward processing. The first stage is the projection from the cortex onto the granular layer of the cerebellum via the mossy fibres; the second stage is the projection from the granule cells onto the Purkinje cells via the parallel fibres. Together, these stages can be approximated by a feedforward network of one hidden layer, where the hidden and output units of the network resemble granule and Purkinje cells, respectively. In particular, the cerebellar computation is\n",
    "\n",
    "$c_{t} = \\mathcal{C}(f(h_{t-1})) = W_{\\mathrm{PF}}f^{\\mathcal{C}}\\left(W_{\\mathrm{MF}}f(h_{t-1})\\right)$\n",
    "\n",
    "where $W_{\\mathrm{MF}}$, $W_{\\mathrm{PF}}$ denote the mossy fibre, parallel fibre weights respectively, and $f^{\\mathcal{C}}$ denotes the cerebellar non-linearity which we set as $f^{\\mathcal{C}}(x) = \\mathrm{ReLU}(x)$. \n",
    "\n",
    "### Cortical plasticity rules\n",
    "\n",
    "As stated earlier, we're interested in how the brain can learn time-dependent tasks. Here we're going to be considering the supervised learning paradigm in which some external 'teacher' provides the desired output to the model. Let $y_t$ denote the desired output at time $t$. The goal of the model then is to minimise the error between the cortical output and desired output, $E_t = \\mathcal{E}(y_t, z_t)$, where $\\mathcal{E}$ is the task error function (e.g. mean-squared error for regression tasks, cross entropy loss for classification tasks). \n",
    "\n",
    "Let $E = \\sum_t E_t$ denote the task error across the entire task sequence. To minimise $E$, we will update our cortical parameters by gradient descent. That is, for a given cortical weight $W$\n",
    "\n",
    "$\\Delta W = - \\eta \\frac{\\partial E}{\\partial W}$\n",
    "\n",
    "Where $\\eta$ is the learning rate. Though $\\frac{\\partial E}{\\partial W}$ is relatively simply to compute for the cortical readout weight $W = W_{\\mathrm{rdt}}$ (one-step backprop in space), solving this gradient for the parameters in the RNN can be challenging. In particular, determining the true gradient normally uses the backpropagation through time (BPTT) algorithm, the computational and memory requirements of which are generally considered [biologically implausible](https://www.sciencedirect.com/science/article/pii/S0959438818302009) in the brain. For this reason, when assumed plastic, we instead use forward-propagating, biologically plausible eligibility traces as in the [eprop algorithm](https://www.nature.com/articles/s41467-020-17236-y). However, as we will see, cerebellar feedback can alleviate the need for plasticity in the cortical RNN at all. \n",
    "\n",
    "### Cerebellar plasticity rules\n",
    "\n",
    "How does the cerebellar network learns? The parallel fibre weights $W_{\\mathrm{PF}}$ are updated so that the cerebellum predicts *future* task targets. Specifically, the cerebellar error is $E^\\mathcal{C}_t = \\mathcal{E}(y_t, c_{t-\\tau})$ for some cerebellar time window $\\tau$. This appeals to the observed timed plasticity rules at the parallel fibre synapse, for which $\\tau$ effectively varies in the hundreds of milliseconds. Moreover, the predictive element of cerebellar output appeals to the notion of the cerebellum as an [internal model](https://www.sciencedirect.com/science/article/pii/S1364661398012212) (specifically forward model) of the nervous system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b8e2f",
   "metadata": {},
   "source": [
    "## Required libraries and functions\n",
    "Let's load the required python libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8611ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import ignite\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sacred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975cd4b",
   "metadata": {},
   "source": [
    "... and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d834c5a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mingredients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linedraw \u001b[38;5;28;01mas\u001b[39;00m dataset, load_linedraw \u001b[38;5;28;01mas\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mingredients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model, init_model\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mingredients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training, init_metrics, init_optimizer, \\\n\u001b[1;32m      6\u001b[0m                                  create_rnn_trainer, create_rnn_evaluator, \\\n\u001b[1;32m      7\u001b[0m                                  Tracer, ModelCheckpoint\n",
      "File \u001b[0;32m~/Documents/other/workshops/mainz_2023/labs/ccLoops-main/scripts/ingredients/training.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src_dir \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath:\n\u001b[1;32m     11\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, src_dir)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandlers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tracer, ModelCheckpoint\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_metrics\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_optimizer, init_lr_scheduler\n",
      "File \u001b[0;32m~/Documents/other/workshops/mainz_2023/labs/ccLoops-main/src/training/handlers.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mignite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandlers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhdlrs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mignite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Events\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelCheckpoint\u001b[39;00m(hdlrs\u001b[38;5;241m.\u001b[39mModelCheckpoint):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "# Load experiment ingredients\n",
    "from ingredients.dataset import linedraw as dataset, load_linedraw as load_dataset\n",
    "    \n",
    "from ingredients.model import model, init_model\n",
    "from ingredients.training import training, init_metrics, init_optimizer, \\\n",
    "                                 create_rnn_trainer, create_rnn_evaluator, \\\n",
    "                                 Tracer, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348a172",
   "metadata": {},
   "source": [
    "## Model initialisation \n",
    "Inspired by the vast number of granule cells in the cerebellum (they constitute $>50\\%$ of the brain's neurons alone!), we apply a significant cortico-cerebellar expansion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
