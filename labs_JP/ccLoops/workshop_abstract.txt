Sparsity and compression at the cerebellar input:

In our more recent work we propose that the cerebellum 'drives' cortical task dynamics. In particular, we model a cortical RNN communicating with a cerebellar feedforward network via the cortico-cerebellar loop. In this work the idea is that cerebellar plasticity (specifically, at what we analogise to the parallel fibre synapse') can actually be enough to acquire good cortical hidden representations with respect to the task. The model appeals to the cerebellum specifically because it has these special timed plasticity rules and large hidden granular layer which enhances its power for pattern separation. 
However, there are important biological features which, sadly, are not currently modeled. This project is interested in two specific features. The first feature is the *sparsity* of cerebellar connectivity; a well known anatomical observation of the cerebellum is that each granule cells receives very sparse input, only about 4 mossy fibres (the external input to the cerebellum) each. People have studied how this sparsity is important for better pattern separation (see e.g. Cayco-Gayic Nat comms 2017, Litwin-Kumar Neuron 2017). The second feature is the *compression* of representation which serves as the input to the cerebellum. In the current model, the *entire* cortical population is given to the cerebellum. This is actually biologically unrealistic, since there is no direct cortico-cerebellar projection, but only an indirect projection via the pontine nuclei. The pontine nuclei are much smaller than the cortex, so the idea is that this could serve as an important compression stage in the brain. I don't know much about the role of compression in general, but I think this paper (https://www.biorxiv.org/content/10.1101/2022.02.10.480014v1) would be very interesting to look at and perhaps replicate within the context of the cortico-cerebellar loop. 
The project is in your hands (do what you want!), but a possible way to do this project is go through the following items (though I would be amazed if you could do all of them in so little time!)

1. Implement sparsity in the cerebellar component of the model (this is actually in some way already implemented, but you might want to change it...). Is there any functional benefit on learning? Metabolic cost? Does GC activation change? (I'm not sure but this may be hard, do say/stop if you feel like it!)
2. Implement compression (i.e. the pontine nuclei) in the cortico-cerebellar projection of the model (this is actually in some way already implemented, but you might want to change it...). How does random projection do? Can it be more optimal? The Litwin Kumar paper above should help. 
3. Any specific advantage of 1. vs 2.? Either important for specific types of task (e.g. motor/continuous vs cognitive/classification)?

1. and 2. can probably be whole projects by themselves, so so feel free to concentrate on 1. or 2. alone depending on your interests (or anything else you might want to with respect to optimising cerebellar processing!)

Note that the code basis for this project can be found in labs_JP/ccLoops on the workshop github page: https://github.com/meggl23/BioDLWorkshop
