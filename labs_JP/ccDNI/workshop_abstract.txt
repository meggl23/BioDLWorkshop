Low rank error gradients:

In much of machine learning/computational neuroscience, network parameters update according to their error gradient : i.e. change to W = -dE/dW. In my work -- in the 'feedback decoupling' paper -- we proposed that in the brain the cerebellum could provide predictions of error gradients to the neocortex; that is, that C(h) ~ dE/dh, where E is the task error, h is the cortical state and C is the cerebellar computation. However, this work makes a strong assumption that the cerebellum predicts individual error gradients for *every* cortical neuron. Besides being computationally expensive this is probably biologically unrealistic: we know the cerebellar nuclei (the output of the cerebellum) are very small with not many neurons; we also know the thalamus, which is the relay station between the cerebellum and cortex, is relatively low dimensional. 
Motivated by this, the proposed project seeks to understand whether succesful learning can happen using *low-rank error gradients*. Specifically, can the error gradient be decomposed into the product of a low-dimensional representation (i.e. thalamus) and projection weights (i.e. thalamo-cortical projection)? It is useful to note that this is done with neural activities all the time, e.g. h = z * W_p, where z is the low-dimensional representation of neural activity (e.g. via autoencoders of PCA) and W_p is the projection weight. Now we are asking whether we can do the same with gradients: i.e. dE/dh = m * W_p, where m is the low-dimensional error gradient. In this scheme, we can now have C(h) = m and fit better to the bottleneck constraints of cerebellar/thalamic nuclei size.
The project is in your hands (do what you want!), but a possible way to do this project is go through the following items (though I would be amazed if you could do all of them in so little time!):

1. Do check the literature! After my brief check it seems like this idea has not been considered much/at all, but worth looking at.
2. Do low dimensional error gradients make sense? i.e. can you learn z, W_p such that z*W_p is good enough for succesful weight updates? What dimensionality of m is required? How does it correspond to the dimensionality of z? I would just test this first on a simple task with a feedforward network, but if that works would be cool to do with an RNN (I think this will be harder but not sure)
3. See if the 'decoupled neural interfaces' (DNI; see Jadeberg et al. 2017) idea (as we use in our paper) works with the cerebellar network predicting m. Again I think it's easier to do this with feedforward networks. I think the original DNI paper would be more helpful to understand the details (especially for FF nets)
4. Do 3. with RNNs.
5. (well done for getting this far!) ask me about my new algorithm for better learning to predict gradients for RNNs and see if it's possible to apply there!

Note that the code basis for this project can be found in labs_JP/ccDNI on the workshop github page: https://github.com/meggl23/BioDLWorkshop

